import asyncio
import sys
import os
import csv
import pandas as pd
import random
import time
from datetime import datetime
from dotenv import load_dotenv
from playwright.async_api import async_playwright

# ‡∏•‡∏≠‡∏á import yfinance (‡∏ó‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢)
try:
    import yfinance as yf
    HAS_YFINANCE = True
except ImportError:
    HAS_YFINANCE = False
    print("‚ö†Ô∏è  Warning: 'yfinance' library not found. Will use Table Scraping only.")

# ==========================================
# 0. SETUP PATHS & IMPORTS
# ==========================================
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, "../../.."))
if project_root not in sys.path:
    sys.path.append(project_root)

# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô ASSET_TYPE ‡πÄ‡∏õ‡πá‡∏ô 'fund' ‡∏´‡∏£‡∏∑‡∏≠ 'etf' ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
ASSET_TYPE = 'fund' 

from src.utils.path_manager import VAL_YF_HIST, VAL_YF_MASTER
from src.utils.logger import setup_logger, log_execution_summary

# ==========================================
# 1. CONFIGURATION
# ==========================================
load_dotenv()
current_date = datetime.now().strftime('%Y-%m-%d')
OUTPUT_DIR = VAL_YF_HIST / "Price_History" / current_date / ASSET_TYPE
ERROR_SCREENSHOT_DIR = OUTPUT_DIR / "errors_screenshots"
ERROR_SCREENSHOT_DIR.mkdir(parents=True, exist_ok=True)

logger = setup_logger("YF_Hybrid_Scraper")

# ==========================================
# 2. HELPER FUNCTIONS
# ==========================================

def download_via_yfinance(ticker):
    """‡∏ó‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢: ‡πÉ‡∏ä‡πâ Library ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å/‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏á‡πâ‡∏≠‡∏õ‡∏∏‡πà‡∏°)"""
    if not HAS_YFINANCE: return None
    try:
        logger.info(f"‚ö° Using yfinance library for {ticker}...")
        # period="max" ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏≠‡∏≤‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ß‡∏±‡∏ô‡πÅ‡∏£‡∏Å
        df = yf.download(ticker, period="max", progress=False, auto_adjust=False)
        
        if not df.empty:
            # ‡∏à‡∏±‡∏î Format ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡∏õ‡∏Å‡∏ï‡∏¥
            df = df.reset_index()
            # ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô format ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
            df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')
            # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Column ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']
            # yfinance ‡∏≠‡∏≤‡∏à‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ MultiIndex ‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á
            df.columns = [c[0] if isinstance(c, tuple) else c for c in df.columns]
            
            # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ
            available_cols = [c for c in cols if c in df.columns]
            return df[available_cols]
    except Exception as e:
        logger.warning(f"   ‚ö†Ô∏è yfinance failed: {e}")
    return None

async def scrape_table_via_playwright(context, ticker):
    """‡∏ó‡∏≤‡∏á‡∏ñ‡∏∂‡∏Å: ‡πÄ‡∏õ‡∏¥‡∏î Browser ‡πÑ‡∏õ‡∏Å‡∏ß‡∏≤‡∏î‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡∏ä‡πâ‡∏≤‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÅ‡∏ï‡πà‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à)"""
    page = await context.new_page()
    # ‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏£‡∏π‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
    await page.route("**/*.{png,jpg,jpeg,gif,webp,svg,css,woff,woff2}", lambda route: route.abort())
    
    data = []
    try:
        url = f"https://finance.yahoo.com/quote/{ticker}/history"
        logger.info(f"üï∑Ô∏è Scraping Table for {ticker}...")
        await page.goto(url, timeout=60000, wait_until="domcontentloaded")

        # Handle Cloudflare / Popups
        if "Just a moment" in await page.title():
             await asyncio.sleep(15)
        
        # Scroll Loop (‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°)
        # Yahoo ‡πÉ‡∏ä‡πâ Infinite Scroll ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏•‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ
        # ‚ö†Ô∏è ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 20 ‡∏õ‡∏µ ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á Scroll ‡∏ô‡∏≤‡∏ô‡∏°‡∏≤‡∏Å ‡∏ú‡∏°‡∏ï‡∏±‡πâ‡∏á Limit ‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà 50 ‡∏£‡∏≠‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡πâ‡∏≤‡∏á
        last_height = await page.evaluate("document.body.scrollHeight")
        retries = 0
        
        # ‡∏•‡∏≠‡∏á Scroll ‡∏™‡∏±‡∏Å 20-30 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á (‡πÑ‡∏î‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 2-3 ‡∏õ‡∏µ‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á)
        # ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏´‡∏°‡∏î‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏° loop ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å
        for i in range(30): 
            await page.keyboard.press("End")
            await asyncio.sleep(1.0)
            
            new_height = await page.evaluate("document.body.scrollHeight")
            if new_height == last_height:
                retries += 1
                if retries >= 3: break # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏°‡πà‡πÑ‡∏õ‡πÑ‡∏´‡∏ô 3 ‡∏£‡∏≠‡∏ö ‡πÉ‡∏´‡πâ‡∏û‡∏≠
            else:
                retries = 0
                last_height = new_height
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á
        rows = page.locator('table[data-test="historical-prices"] tbody tr')
        count = await rows.count()
        
        if count > 0:
            all_texts = await rows.all_inner_texts()
            for text in all_texts:
                # ‡πÅ‡∏¢‡∏Å Column
                cols = text.split('\t')
                if len(cols) < 5: cols = text.split('\n')
                
                # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ñ‡∏ß‡∏£‡∏≤‡∏Ñ‡∏≤ (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 6-7 ‡∏ä‡πà‡∏≠‡∏á) ‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡πÅ‡∏ñ‡∏ß Dividend
                if len(cols) >= 6:
                    # ‡πÅ‡∏õ‡∏•‡∏á Date ‡∏à‡∏≤‡∏Å "Dec 26, 2025" -> "2025-12-26"
                    try:
                        dt = datetime.strptime(cols[0], "%b %d, %Y")
                        cols[0] = dt.strftime("%Y-%m-%d")
                        data.append(cols[:7])
                    except:
                        pass # ‡∏Ç‡πâ‡∏≤‡∏°‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏Å‡πÜ

    except Exception as e:
        logger.error(f"   ‚ùå Table Scraping Error: {e}")
    finally:
        await page.close()
    
    if data:
        headers = ["Date", "Open", "High", "Low", "Close", "Adj Close", "Volume"]
        return pd.DataFrame(data, columns=headers[:len(data[0])])
    return None

async def process_ticker(context, ticker, progress_str):
    final_df = None
    status = "error"
    
    # 1. ‡∏•‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢ (yfinance library)
    final_df = download_via_yfinance(ticker)
    
    # 2. ‡∏ñ‡πâ‡∏≤‡∏ó‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡∏ñ‡∏∂‡∏Å (Playwright Table)
    if final_df is None or final_df.empty:
        final_df = await scrape_table_via_playwright(context, ticker)
    
    # Save Result
    if final_df is not None and not final_df.empty:
        final_path = OUTPUT_DIR / f"{ticker}_history.csv"
        final_df.to_csv(final_path, index=False)
        logger.info(f"{progress_str} ‚úÖ {ticker}: Saved {len(final_df)} rows.")
        status = "success"
    else:
        logger.warning(f"{progress_str} ‚ùå {ticker}: Failed all methods.")
        status = "not_found"
        
    return status

def get_all_downloaded_tickers(base_path):
    downloaded = set()
    if not base_path.exists(): return downloaded
    for file_path in base_path.rglob("*_history.csv"):
        downloaded.add(file_path.name.replace("_history.csv", ""))
    return downloaded

async def main():
    logger.info(f"üöÄ STARTING: YF HYBRID SCRAPER (LIB + TABLE)")
    start_time = time.time()
    
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # Load Master
    try:
        master_path = list(VAL_YF_MASTER.rglob(f"yf_{ASSET_TYPE}_master.csv"))[-1]
        all_tickers = pd.read_csv(master_path)['ticker'].astype(str).tolist()
    except Exception as e:
        logger.error(f"‚ùå Master list error: {e}")
        return

    # Smart Resume
    done_tickers = get_all_downloaded_tickers(VAL_YF_HIST) 
    queue = [t for t in all_tickers if t not in done_tickers]
    logger.info(f"‚è≠Ô∏è  Skipped: {len(done_tickers)} | ‚ñ∂Ô∏è  Remaining: {len(queue)}")

    if not queue: return

    stats = {"success": 0, "error": 0, "not_found": 0}
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36")

        # üî• Concurrency = 2
        semaphore = asyncio.Semaphore(2)

        async def worker(t, idx):
            async with semaphore:
                prog = f"[{idx}/{len(all_tickers)}]"
                res = await process_ticker(context, t, prog)
                
                if res in stats: stats[res] += 1
                await asyncio.sleep(random.uniform(1, 3))

        tasks = [worker(t, i+1+len(done_tickers)) for i, t in enumerate(queue)]
        await asyncio.gather(*tasks)
        await browser.close()

    log_execution_summary(logger, start_time, sum(stats.values()), "Completed", stats)

if __name__ == "__main__":
    asyncio.run(main())