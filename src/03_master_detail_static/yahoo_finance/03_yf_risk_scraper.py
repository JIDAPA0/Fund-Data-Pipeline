import sys
import asyncio
import pandas as pd
import random
import re
from pathlib import Path
from playwright.async_api import async_playwright

# ==========================================
# SYSTEM PATH SETUP
# ==========================================
current_dir = Path(__file__).resolve().parent
project_root = current_dir.parents[2]
if str(project_root) not in sys.path: 
    sys.path.append(str(project_root))

from src.utils.logger import setup_logger
from src.utils.db_connector import get_active_tickers

# ==========================================
# CONFIGURATION
# ==========================================
logger = setup_logger("03_yf_risk_scraper")

OUTPUT_DIR = project_root / "validation_output" / "Yahoo_Finance" / "03_Detail_Static"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_FILE = OUTPUT_DIR / "yf_fund_risk.csv"

# Mapping: ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö -> ‡∏ä‡∏∑‡πà‡∏≠ prefix ‡πÉ‡∏ô database
metrics_map = {
    "alpha": "Alpha",
    "beta": "Beta",
    "mean_annual_return": "Mean Annual Return",
    "r_squared": "R-Squared",
    "standard_deviation": "Standard Deviation",
    "sharpe_ratio": "Sharpe Ratio",
    "treynor_ratio": "Treynor Ratio"
}

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Columns Header: ticker, rating, alpha_3y, alpha_5y, ...
COLS = ["ticker", "morningstar_rating"]
for m in metrics_map.keys():
    for y in ["3y", "5y", "10y"]:
        COLS.append(f"{m}_{y}")
COLS.append("updated_at")

class YFRiskScraper:
    def __init__(self):
        self.tickers_data = get_active_tickers("Yahoo Finance")
        
        # Resume Logic
        self.processed_tickers = set()
        if OUTPUT_FILE.exists():
            try:
                df = pd.read_csv(OUTPUT_FILE)
                if 'ticker' in df.columns:
                    self.processed_tickers = set(df['ticker'].astype(str))
                logger.info(f"‚è≠Ô∏è Found existing file. Skipping {len(self.processed_tickers)} rows.")
            except: pass

    async def scrape_risk(self, page, ticker):
        # ‡∏ï‡∏±‡∏î Suffix ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤ URL ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å (‡πÄ‡∏ä‡πà‡∏ô VOO:PCQ -> VOO)
        yf_ticker = ticker.split(':')[0]
        url = f"https://finance.yahoo.com/quote/{yf_ticker}/risk"
        
        data = {c: None for c in COLS}
        data["ticker"] = ticker
        data["updated_at"] = pd.Timestamp.now().strftime("%Y-%m-%d")

        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)
            
            # Scroll ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏≤‡∏£‡∏≤‡∏á (Lazy Load)
            await page.evaluate("window.scrollBy(0, 500)")
            await asyncio.sleep(2) # ‡∏£‡∏≠ Animation

            # --- 1. MORNINGSTAR RATING (STARS) ---
            try:
                # ‡∏´‡∏≤ span ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏î‡∏≤‡∏ß ‚òÖ
                stars_elements = await page.locator('span:has-text("‚òÖ")').all_inner_texts()
                if stars_elements:
                    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏î‡∏≤‡∏ß‡πÄ‡∏¢‡∏≠‡∏∞‡∏™‡∏∏‡∏î (‡πÄ‡∏ä‡πà‡∏ô ‚òÖ‚òÖ‚òÖ)
                    rating_str = max(stars_elements, key=lambda x: x.count("‚òÖ"))
                    data["morningstar_rating"] = rating_str.count("‚òÖ")
            except: pass

            # --- 2. RISK METRICS (TABLE STRATEGY) ---
            # ‡πÉ‡∏ä‡πâ Locator ‡∏´‡∏≤‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "Alpha" (‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Å‡∏ß‡πà‡∏≤ data-testid)
            try:
                # ‡∏´‡∏≤ Table ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ Alpha ‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ô
                table_loc = page.locator("table").filter(has_text="Alpha").first
                
                # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß (tr)
                rows = await table_loc.locator("tr").all()
                
                for row in rows:
                    # ‡∏î‡∏∂‡∏á Text ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ß (Label + Values)
                    cells = await row.locator("td").all_inner_texts()
                    if not cells: continue # ‡∏Ç‡πâ‡∏≤‡∏°‡πÅ‡∏ñ‡∏ß header ‡∏´‡∏£‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏á
                    
                    row_label = cells[0].strip() # ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏Ñ‡∏∑‡∏≠ Label (e.g., "Alpha")
                    
                    # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ Label ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Metric ‡πÑ‡∏´‡∏ô‡πÉ‡∏ô map ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÑ‡∏´‡∏°
                    for metric_key, web_label in metrics_map.items():
                        # ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÅ‡∏ö‡∏ö Case Insensitive
                        if web_label.lower() in row_label.lower():
                            # Yahoo Table Columns Format: [Label, 3Y, 5Y, 10Y]
                            # ‡∏ö‡∏≤‡∏á‡∏ó‡∏µ‡∏°‡∏µ Benchmark ‡πÅ‡∏ó‡∏£‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
                            
                            # ‡∏õ‡∏Å‡∏ï‡∏¥: Label | 3Y | 5Y | 10Y
                            if len(cells) >= 2: data[f"{metric_key}_3y"] = cells[1].strip()
                            if len(cells) >= 3: data[f"{metric_key}_5y"] = cells[2].strip()
                            if len(cells) >= 4: data[f"{metric_key}_10y"] = cells[3].strip()
                            
                            break # ‡πÄ‡∏à‡∏≠‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏¢‡∏∏‡∏î‡∏ß‡∏ô map ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ô‡∏µ‡πâ
                            
            except Exception as e:
                # ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ (‡πÄ‡∏ä‡πà‡∏ô ETF ‡∏ö‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ï‡∏≤‡∏£‡∏≤‡∏á Risk) ‡∏Å‡πá‡∏õ‡∏•‡πà‡∏≠‡∏¢‡∏ú‡πà‡∏≤‡∏ô
                pass

            # Log ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            log_beta = data.get('beta_3y', '-')
            log_rating = data.get('morningstar_rating', '-')
            logger.info(f"‚úÖ {ticker}: Rating={log_rating}, Beta(3Y)={log_beta}")
            return data

        except Exception as e:
            logger.error(f"‚ùå {ticker} Error: {e}")
            return None

    async def run(self):
        queue = [t for t in self.tickers_data if t['ticker'] not in self.processed_tickers]
        logger.info(f"üöÄ Risk Scraper Started. Remaining: {len(queue)}")
        
        if not queue: 
            logger.info("üéâ All done! No new tickers.")
            return

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = await context.new_page()
            
            for i, item in enumerate(queue, 1):
                res = await self.scrape_risk(page, item['ticker'])
                if res:
                    df = pd.DataFrame([res])[COLS]
                    use_header = not OUTPUT_FILE.exists()
                    df.to_csv(OUTPUT_FILE, mode='a', header=use_header, index=False)
                
                # Random Delay (‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡πÇ‡∏î‡∏ô‡∏ö‡∏•‡πá‡∏≠‡∏Å)
                await asyncio.sleep(random.uniform(2, 4))
                
                # Restart Context ‡∏ó‡∏∏‡∏Å‡πÜ 20 ‡∏ï‡∏±‡∏ß
                if i % 20 == 0:
                    await context.close()
                    context = await browser.new_context(
                        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
                    )
                    page = await context.new_page()

            await browser.close()
        logger.info("üéâ Risk Scraper Finished!")

if __name__ == "__main__":
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(YFRiskScraper().run())