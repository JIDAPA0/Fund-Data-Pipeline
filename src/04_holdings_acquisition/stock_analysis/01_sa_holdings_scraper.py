import os
import csv
import asyncio
import pandas as pd
import configparser
from datetime import datetime
from pathlib import Path
import time
import random
from playwright.async_api import async_playwright, TimeoutError
from typing import List, Dict, Any, Set

# --- ‚öôÔ∏è CONFIGURATION ---------------------------------------------------------
# Path ‡πÑ‡∏ü‡∏•‡πå Input (Master List)
INPUT_CSV_PATH = "validation_output/Stock_Analysis/01_List_Master/2025-12-03/sa_etf_master.csv"

# Path ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Output
BASE_OUTPUT_DIR = Path("validation_output/Stock_Analysis/04_Holdings")

# Base URL
BASE_URL = "https://stockanalysis.com/etf/"

# üéØ BATCH_SIZE (‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Ticker ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≠‡∏ö‡∏Å‡∏≤‡∏£ Login)
BATCH_SIZE = 500 

# üöÄ NEW: CONCURRENCY SETTING (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Ticker ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏£‡∏±‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô)
MAX_CONCURRENT_TICKERS = 4 # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà 4-5 ‡∏ï‡∏±‡∏ß ‡∏´‡∏≤‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÑ‡∏´‡∏ß ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏î‡πâ

# --- INI Config Reader (‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°) --------------------------------
def get_config(filename='config/database.ini', section='stock_analysis'):
    parser = configparser.ConfigParser()
    if not os.path.exists(filename):
        filename = os.path.join(os.getcwd(), filename)
        if not os.path.exists(filename):
             raise FileNotFoundError(f"Configuration file not found at: {filename}")
    parser.read(filename)
    if parser.has_section(section):
        return dict(parser.items(section))
    else:
        raise Exception(f'Section {section} not found in the {filename} file')

try:
    CONFIG = get_config()
except Exception as e:
    print(f"FATAL ERROR: Failed to load configuration. Error: {e}")
    exit(1) 

LOGIN_URL = CONFIG.get('login_url')
EMAIL = CONFIG.get('email')
PASS = CONFIG.get('password')


# --- Utility Functions ----------------------------------------------------

# üõ†Ô∏è ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Tickers ‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡πâ‡∏ß (‡∏£‡∏±‡∏ô‡∏ï‡πà‡∏≠)
def get_processed_tickers(target_dir: Path) -> Set[str]:
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå Output"""
    if not target_dir.exists():
        return set()
    
    processed_files = target_dir.glob("*_holdings.csv") 
    processed_tickers = set()
    
    for file_path in processed_files:
        ticker = file_path.name.split('_holdings.csv')[0]
        if file_path.stat().st_size > 0:
             processed_tickers.add(ticker)
            
    return processed_tickers

# üõ†Ô∏è Login Function (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Persistent Context)
async def login_to_sa(page):
    """Login ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö"""
    print(f"üîê Attempting Login as {EMAIL}...")
    try:
        await page.goto(LOGIN_URL, wait_until="domcontentloaded", timeout=30000)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏ô‡πâ‡∏≤ Login ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if "login" in page.url:
            await page.fill('input[type="email"]', EMAIL)
            await page.fill('input[type="password"]', PASS)
            await page.keyboard.press("Enter")
            
            # ‡∏£‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ Login ‡πÅ‡∏•‡πâ‡∏ß
            await page.wait_for_url(lambda url: "login" not in url, timeout=30000)
            
            if "login" not in page.url:
                print("‚úÖ Login Successful!")
                return True 
            else:
                print("‚ùå Login Failed (Still on login page)")
                return False 
        else:
            # ‡∏≠‡∏≤‡∏à‡∏à‡∏∞ Login ‡∏Ñ‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
            print("‚úÖ Session already authenticated or not required.")
            return True

    except Exception as e:
        print(f"‚ùå Critical Login Error: {e}")
        return False


async def download_holdings(page, ticker, target_dir):
    """‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤ Holdings ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏î Download CSV ‡∏•‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î"""
    url = f"{BASE_URL}{ticker.lower()}/holdings/"
    save_path = target_dir / f"{ticker}_holdings.csv" 
    
    try:
        await page.goto(url, wait_until="domcontentloaded", timeout=60000)
        download_btn = page.locator('button:has-text("Download")')
        
        if await download_btn.count() > 0 and await download_btn.first.is_visible():
            await download_btn.first.click()
            csv_option = page.locator('button:has-text("Download to CSV"), div[role="menu"] button:has-text("Download to CSV")')
            
            try:
                await csv_option.first.wait_for(state="visible", timeout=3000)
            except:
                await download_btn.first.click()
                await asyncio.sleep(0.5)

            async with page.expect_download(timeout=15000) as download_info:
                await csv_option.first.click(force=True)
            
            download = await download_info.value
            await download.save_as(save_path)
            
            if save_path.exists() and save_path.stat().st_size > 0:
                return True
        else:
            pass 

    except Exception as e:
        pass
    
    return False

def generate_report(output_dir, start_time, total, success, skipped):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á Report ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• (‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°)"""
    end_time = time.time()
    minutes = int((end_time - start_time) // 60)
    seconds = (end_time - start_time) % 60
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = output_dir / f"Report_Holdings_{timestamp}.txt"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("============================================================\n")
        f.write("üìä  SCRAPING REPORT: ETF HOLDINGS (DATE FOLDER)\n")
        f.write("============================================================\n")
        f.write(f"üóìÔ∏è  Execution Date : {datetime.now().strftime('%d %B %Y, %H:%M:%S')}\n")
        f.write(f"üìÇ  Data Location  : {output_dir}\n")
        f.write("-" * 60 + "\n")
        f.write(f"üîπ Total Tickers       : {total:,}\n")
        f.write(f"‚úÖ Downloaded          : {success:,}\n")
        f.write(f"‚ö†Ô∏è  No Data / Skipped   : {skipped:,}\n")
        f.write(f"‚è±Ô∏è  Time Taken          : {minutes}m {seconds:.2f}s\n")
        f.write("============================================================\n")
    print(f"\nüìù Report: {report_path}")


# üõ†Ô∏è NEW: Worker function for concurrent processing
async def worker(ticker: str, context, TODAY_DIR: Path, all_tickers: List[str], counters: Dict[str, Any]):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Worker ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Ticker (‡πÉ‡∏ä‡πâ Concurrency)"""
    
    # ‚ö†Ô∏è ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á page ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ worker
    page = await context.new_page()
    
    try:
        # üîí ‡πÉ‡∏ä‡πâ Lock ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ï‡∏±‡∏ß‡∏ô‡∏±‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        async with counters['lock']:
            counters['total_count'] += 1
            current_index = counters['total_count']
            
        print(f"[{current_index}/{len(all_tickers)}] üì• Holdings: {ticker} ... ", end='', flush=True)

        is_saved = await download_holdings(page, ticker, TODAY_DIR)
        
        async with counters['lock']:
            if is_saved:
                counters['success_count'] += 1
                print(f"‚úÖ Saved")
            else:
                counters['skipped_count'] += 1
                print(f"‚ö†Ô∏è  No Data")
        
    except Exception as e:
        print(f"üö® Worker Error for {ticker}: {e}")
        async with counters['lock']:
            counters['skipped_count'] += 1
            
    finally:
        await page.close()


# --- MAIN LOGIC (‡πÉ‡∏ä‡πâ Persistent Context ‡πÅ‡∏•‡∏∞ Concurrency) -----------------------
async def main():
    print("\n--- üöÄ STARTING HOLDINGS DOWNLOADER (SPEED MODE) ---")
    start_time = time.time()
    
    # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
    today_str = datetime.now().strftime('%Y-%m-%d')
    TODAY_DIR = BASE_OUTPUT_DIR / today_str
    TODAY_DIR.mkdir(parents=True, exist_ok=True)
    
    print(f"üìÇ Target Folder Created: {TODAY_DIR}") 

    # 2. Load Tickers ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Tickers ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß
    try:
        df = pd.read_csv(INPUT_CSV_PATH)
        all_tickers = df['ticker'].tolist()
        processed_tickers = get_processed_tickers(TODAY_DIR)
        tickers_to_process = [t for t in all_tickers if t not in processed_tickers]
        
        print(f"üìÑ Loaded {len(all_tickers)} total tickers.")
        print(f"üíæ Found {len(processed_tickers)} tickers already processed (Skipping).")
        print(f"‚è≥ {len(tickers_to_process)} tickers remaining to process.")
        
        if not tickers_to_process:
            print("üéâ All tickers for today are already processed. Exiting.")
            return
        
    except Exception as e:
        print(f"‚ùå Error reading Master CSV: {e}")
        return

    # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Counters ‡πÅ‡∏•‡∏∞ Lock ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Parallel Run
    counters = {
        'total_count': len(processed_tickers), # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ô‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß
        'success_count': 0,
        'skipped_count': 0,
        'lock': asyncio.Lock()
    }
    initial_processed_count = len(processed_tickers)
    
    async with async_playwright() as p:
        # üöÄ ‡πÉ‡∏ä‡πâ Persistent Context ‡πÄ‡∏û‡∏∑‡πà‡∏≠ Login ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        user_data_dir = "./tmp/sa_session"
        context = await p.chromium.launch_persistent_context(
            user_data_dir=user_data_dir,
            headless=True,
            accept_downloads=True
        )

        # 4. Login ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Session (‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
        page = await context.new_page()
        if not await login_to_sa(page):
            await context.close()
            print("üö® CRITICAL: Initial Login Failed. Please check credentials or wait for IP unblock.")
            return
        await page.close() 

        print(f"\n--- Starting Data Acquisition with {MAX_CONCURRENT_TICKERS} workers ---")

        # 5. ‡∏£‡∏±‡∏ô Tickers ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Concurrency
        tasks = []
        for ticker in tickers_to_process:
            tasks.append(worker(ticker, context, TODAY_DIR, all_tickers, counters))

        # 6. ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Batch ‡∏ï‡∏≤‡∏° Concurrency Limit
        for i in range(0, len(tasks), MAX_CONCURRENT_TICKERS):
            batch = tasks[i:i + MAX_CONCURRENT_TICKERS]
            await asyncio.gather(*batch)
            
            # üí° ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Delay ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Batch ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÉ‡∏ä‡πâ Concurrency ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
            # ‡πÅ‡∏ï‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏™‡πà delay ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô CPU Overload
            # await asyncio.sleep(0.5) 

        await context.close()

    # 7. Final Report
    final_success_count = initial_processed_count + counters['success_count']
    final_skipped_count = counters['skipped_count']

    generate_report(BASE_OUTPUT_DIR, start_time, len(all_tickers), final_success_count, final_skipped_count)
    print("\n--- üèÅ ALL OPERATIONS COMPLETED ---")

if __name__ == "__main__":
    asyncio.run(main())